import whisper
import json
import os
import torch
import subprocess
import re

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n c·ªë ƒë·ªãnh cho model
MODEL_DIR = "models"
MODEL_PATHS = {
    "tiny": os.path.join(MODEL_DIR, "tiny.pt"),
    "base": os.path.join(MODEL_DIR, "base.pt"),
    "small": os.path.join(MODEL_DIR, "small.pt"),
    "medium": os.path.join(MODEL_DIR, "medium.pt"),
    "large": os.path.join(MODEL_DIR, "large.pt")
}

def split_text_by_chars(text, max_chars):
    """Chia vƒÉn b·∫£n th√†nh c√°c ph·∫ßn c√≥ ƒë·ªô d√†i k√Ω t·ª± ph√π h·ª£p."""
    if len(text) <= max_chars:
        return [text]
    
    # T√¨m v·ªã tr√≠ c·∫Øt ph√π h·ª£p (sau d·∫•u c√¢u ho·∫∑c kho·∫£ng tr·∫Øng)
    parts = []
    current_text = text
    
    while len(current_text) > max_chars:
        # T√¨m v·ªã tr√≠ c·∫Øt ph√π h·ª£p
        cut_pos = max_chars
        for i in range(max_chars, 0, -1):
            if current_text[i] in '.,!?;: ':
                cut_pos = i + 1
                break
        
        # N·∫øu kh√¥ng t√¨m th·∫•y d·∫•u c√¢u, c·∫Øt t·∫°i max_chars
        if cut_pos == max_chars:
            cut_pos = max_chars
        
        # Th√™m ph·∫ßn ƒë√£ c·∫Øt v√†o danh s√°ch
        parts.append(current_text[:cut_pos].strip())
        current_text = current_text[cut_pos:].strip()
    
    if current_text:
        parts.append(current_text)
    
    return parts

def adjust_segment_duration(segment, min_duration, max_duration):
    """ƒêi·ªÅu ch·ªânh th·ªùi l∆∞·ª£ng c·ªßa segment ƒë·ªÉ ph√π h·ª£p v·ªõi gi·ªõi h·∫°n."""
    duration = segment["end"] - segment["start"]
    if duration < min_duration:
        segment["end"] = segment["start"] + min_duration
    elif duration > max_duration:
        segment["end"] = segment["start"] + max_duration
    return segment

def check_ffmpeg():
    try:
        subprocess.run(['ffmpeg', '-version'], capture_output=True, check=True)
        return True
    except FileNotFoundError:
        print("\n‚ö†Ô∏è FFmpeg not found!")
        print("üîπ Please install FFmpeg:")
        print("1. Download FFmpeg from: https://github.com/BtbN/FFmpeg-Builds/releases")
        print("2. Extract the downloaded zip file")
        print("3. Add FFmpeg's bin folder to your system's PATH")
        print("4. Restart your terminal/IDE")
        print("\nOr install via package manager:")
        print("- Windows (using Chocolatey): choco install ffmpeg")
        print("- Windows (using Scoop): scoop install ffmpeg")
        print("- Windows (using Winget): winget install ffmpeg")
        return False

def format_timestamp(seconds):
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millisecs = int((seconds - int(seconds)) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"

def check_device():
    if device == "auto":
        if torch.cuda.is_available():
            print("üîç Detected NVIDIA GPU, using CUDA")
            return "cuda"
        elif torch.backends.mps.is_available():
            print("üîç Detected Apple M1/M2 GPU, using MPS")
            return "mps"
        else:
            print("üîç No GPU detected, using CPU")
            return "cpu"
    elif device == "cuda":
        if torch.cuda.is_available():
            print("üîç Using NVIDIA GPU as requested")
            return "cuda"
        else:
            print("‚ö†Ô∏è NVIDIA GPU not detected!")
            print("üîπ Please install PyTorch with CUDA support using:\n")
            print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n")
            print("Or visit: https://pytorch.org/get-started/locally/")
            exit(1)
    elif device == "mps":
        if torch.backends.mps.is_available():
            print("üîç Using Apple M1/M2 GPU as requested")
            return "mps"
        else:
            print("‚ö†Ô∏è Apple M1/M2 GPU not detected!")
            return "cpu"
    else:
        print("üîç Using CPU as requested")
        return "cpu"

# Ki·ªÉm tra FFmpeg tr∆∞·ªõc khi ti·∫øp t·ª•c
if not check_ffmpeg():
    exit(1)

# T·∫°o th∆∞ m·ª•c models n·∫øu ch∆∞a t·ªìn t·∫°i
os.makedirs(MODEL_DIR, exist_ok=True)

# ƒê·ªçc c·∫•u h√¨nh t·ª´ file config.json
with open("config.json", "r", encoding="utf-8") as f:
    config = json.load(f)

# L·∫•y th√¥ng tin t·ª´ config
input_file = config["input_file"]
output_format = config["output_format"]
language = config["language"]
model_name = config["model"]
device = config["device"]
translate = config.get("translate", False)
task = config.get("task", "transcribe")
initial_prompt = config.get("initial_prompt", "")

# L·∫•y c√°c t√πy ch·ªçn ph·ª• ƒë·ªÅ
subtitle_options = config.get("subtitle_options", {})
max_chars = subtitle_options.get("max_chars", 80)

# Ch·ªçn thi·∫øt b·ªã
selected_device = check_device()
print(f"‚úÖ Using device: {selected_device}")

# Ki·ªÉm tra file ƒë·∫ßu v√†o c√≥ t·ªìn t·∫°i kh√¥ng
if not os.path.exists(input_file):
    print(f"Error: File '{input_file}' does not exist!")
    exit(1)

# Load m√¥ h√¨nh Whisper
print(f"üîπ Loading Whisper model: {model_name}...")
if os.path.exists(MODEL_PATHS[model_name]):
    print(f"‚úÖ Model already exists, loading from {MODEL_PATHS[model_name]}")
else:
    print(f"‚è≥ Downloading model {model_name}...")

model = whisper.load_model(model_name, device=selected_device, download_root=MODEL_DIR)

# Nh·∫≠n di·ªán gi·ªçng n√≥i v·ªõi c√°c t√πy ch·ªçn n√¢ng cao
print(f"üé§ Transcribing audio from '{input_file}'...")
result = model.transcribe(
    input_file,
    language=language,
    task=task,
    initial_prompt=initial_prompt,
    verbose=True
)

# L∆∞u k·∫øt qu·∫£ ra file v·ªõi ƒë·ªãnh d·∫°ng ph√π h·ª£p
if output_format == "srt":
    output_file = "output.srt"
    subtitle_index = 1
    with open(output_file, "w", encoding="utf-8") as f:
        for segment in result["segments"]:
            start_time = format_timestamp(segment["start"])
            end_time = format_timestamp(segment["end"])
            text = segment["text"].strip()
            
            # Chia vƒÉn b·∫£n th√†nh c√°c ph·∫ßn n·∫øu c·∫ßn
            if len(text) > max_chars:
                parts = split_text_by_chars(text, max_chars)
                for part in parts:
                    f.write(f"{subtitle_index}\n")
                    f.write(f"{start_time} --> {end_time}\n")
                    f.write(f"{part}\n\n")
                    subtitle_index += 1
            else:
                f.write(f"{subtitle_index}\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{text}\n\n")
                subtitle_index += 1
elif output_format == "vtt":
    output_file = "output.vtt"
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("WEBVTT\n\n")
        for segment in result["segments"]:
            start_time = format_timestamp(segment["start"]).replace(",", ".")
            end_time = format_timestamp(segment["end"]).replace(",", ".")
            text = segment["text"].strip()
            
            # Chia vƒÉn b·∫£n th√†nh c√°c ph·∫ßn n·∫øu c·∫ßn
            if len(text) > max_chars:
                parts = split_text_by_chars(text, max_chars)
                for part in parts:
                    f.write(f"{start_time} --> {end_time}\n")
                    f.write(f"{part}\n\n")
            else:
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{text}\n\n")
else:
    output_file = "output.txt"
    with open(output_file, "w", encoding="utf-8") as f:
        for segment in result["segments"]:
            start_time = format_timestamp(segment["start"])
            end_time = format_timestamp(segment["end"])
            text = segment["text"].strip()
            
            # Chia vƒÉn b·∫£n th√†nh c√°c ph·∫ßn n·∫øu c·∫ßn
            if len(text) > max_chars:
                parts = split_text_by_chars(text, max_chars)
                for part in parts:
                    f.write(f"[{start_time} --> {end_time}] {part}\n")
            else:
                f.write(f"[{start_time} --> {end_time}] {text}\n")

print(f"‚úÖ Content has been saved to '{output_file}' üéâ")
